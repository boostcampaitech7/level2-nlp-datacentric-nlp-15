{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "373bf38cdc6b41109be13ef3f5ebe0c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ExaoneForCausalLM(\n",
       "  (transformer): ExaoneModel(\n",
       "    (wte): Embedding(102400, 4096, padding_idx=0)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-31): 32 x ExaoneBlock(\n",
       "        (ln_1): ExaoneRMSNorm()\n",
       "        (attn): ExaoneAttention(\n",
       "          (attention): ExaoneSelfAttention(\n",
       "            (rotary): ExaoneRotaryEmbedding()\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): ExaoneRMSNorm()\n",
       "        (mlp): ExaoneGatedMLP(\n",
       "          (c_fc_0): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (c_fc_1): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (c_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): ExaoneRMSNorm()\n",
       "    (rotary): ExaoneRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=102400, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\" #\"nlpai-lab/KULLM3\" #\"rtzr/ko-gemma-2-9b-it\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True \n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/filtered_exaone_restored.csv\", \"r\") as f:\n",
    "    df = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_topic(text: str):\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \n",
    "    \"content\": \"당신은 신문 기자입니다. 입력으로 같은 target 라벨을 가지고 있는 'ID' 'text' 'target'으로 이루어진 텍스트들 여러 개가 들어올 겁니다. 해당 입력의 target의 공통 주제 단 한가지를 들어온 텍스트들의 분석을 통해 유추하시오. 유추할 때는 모든 텍스트들을 고려하여서 가장 포괄적인 키워드를 뽑아야합니다. 키워드는 뉴스 카테고리 중 하나입니다. 설명을 붙이지 말고 유추한 키워드 단 한 개만을 출력하세요.\"},\n",
    "    {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    outputs = llm.generate(\n",
    "        inputs.to(device),\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # print(generated_text)\n",
    "\n",
    "    result = generated_text.split(\"[|assistant|]\")[-1].strip()\n",
    "    if '\\n' in result:\n",
    "        result = result.split(\"\\n\")[0]\n",
    "    print(result)\n",
    "    del inputs, outputs, generated_text\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(text: str, subject: str):\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \n",
    "    \"content\": \"당신은 신문 기자입니다. 입력으로 같은 target 라벨을 가지고 있는 'ID' 'text' 'target'으로 이루어진 텍스트들 여러 개가 들어오고 그들의 공통 주제가 주어집니다. 들어온 텍스트들 중에는 공통 주제에 맞지 않는 텍스트들이 존재하는데 이들을 필터링하는 임무를 부여받았습니다. 당신은 주어진 텍스트들에서 이들을 정제한 후 그 결과를 설명을 붙이지 말고 'ID','text','target' 형태로 출력하세요.\"},\n",
    "    {\"role\": \"user\", \"content\": text},\n",
    "    {\"role\": \"user\", \"content\": subject}\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    outputs = llm.generate(\n",
    "        inputs.to(device),\n",
    "        max_new_tokens=4096,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # print(generated_text)\n",
    "\n",
    "    result = generated_text.split(\"[|assistant|]\")[-1].strip()\n",
    "    del inputs, outputs, generated_text\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6952999c0b74545bbab6df17e59baa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "label:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748e5e13e7dc4b55814915cf152b1fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter mis-labels:   0%|          | 0/233 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "날씨\n",
      "문화\n",
      "힐링\n",
      "문화\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1daab34ff6934dd8a160e989e1853a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter mis-labels:   0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "스포츠\n",
      "스포츠\n",
      "축구\n",
      "스포츠\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927711b8b262465689eb9787dcb004d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter mis-labels:   0%|          | 0/232 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정치\n",
      "정치\n",
      "정치\n",
      "정치\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "619da7835e364341a4ae2759335d03b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter mis-labels:   0%|          | 0/224 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "경제\n",
      "사회\n",
      "경제\n",
      "정치\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5adfb25beea4fa0a4d95622284ddc49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter mis-labels:   0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기술\n",
      "기술 혁신\n",
      "기술 혁신\n",
      "기술 혁신\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e6057efa2d458ab05fe7dcaef74e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter mis-labels:   0%|          | 0/230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "경제\n",
      "경제\n",
      "경제\n",
      "경제\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eea9b053672435c9d8d2be9b9dced7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter mis-labels:   0%|          | 0/222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정치\n",
      "정치\n",
      "국제 정치\n",
      "정치\n",
      "['문화', '스포츠', '정치', '사회', '기술', '경제', '국제']\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "filtered_datas = []\n",
    "topics_dict = []\n",
    "for i in tqdm(range(0,7), desc=\"label\"):\n",
    "    rows = df[df['target'] == i].sample(frac=1).reset_index(drop=True)\n",
    "    keywords = {}\n",
    "    context = \"\"\n",
    "    for k, row in tqdm(rows.iterrows(), desc=\"Filter mis-labels\", total=len(rows)):\n",
    "        context += f\"{row['ID']},{row['text']},{row['target']}\\n\"\n",
    "        if (k + 1) % 60 == 0 or k == len(rows) - 1:\n",
    "            topic = inference_topic(context)\n",
    "            sub_topic = topic.split(\" \")\n",
    "            for sub in sub_topic:\n",
    "                keywords[sub] = keywords.get(sub, 0) + 1\n",
    "            context = \"\"\n",
    "    keywords = dict(sorted(keywords.items(), key=lambda item: item[1], reverse=True))\n",
    "    topics_dict.append(keywords)\n",
    "    # for key in keywords.keys():\n",
    "    #     if key not in topics:\n",
    "    #         topics.append(key)\n",
    "    #         break\n",
    "\n",
    "topics = []\n",
    "for i, sub1 in enumerate(topics_dict):\n",
    "    nan = []\n",
    "    candidates = []\n",
    "    can_val = []\n",
    "    for k, sub2 in enumerate(topics_dict):\n",
    "        if i == k: continue\n",
    "        skip_outer = False\n",
    "        for key1 in sub1.keys():\n",
    "            if key1 in topics: continue\n",
    "            if key1 not in sub2.keys():\n",
    "                if key1 not in candidates:\n",
    "                    candidates.append(key1)\n",
    "                    can_val.append(sub1[key1])\n",
    "                break\n",
    "            if key1 in sub2.keys():\n",
    "                if sub1[key1] < sub2[key1]: \n",
    "                    if k > i: nan.append(key1)\n",
    "                    continue\n",
    "                if key1 not in candidates:\n",
    "                    candidates.append(key1)\n",
    "                    can_val.append(sub1[key1])\n",
    "\n",
    "    mx = -1\n",
    "    fnl = None\n",
    "    for v, can in enumerate(candidates):\n",
    "        if can in nan: continue\n",
    "        if mx < v: \n",
    "            mx = v\n",
    "            fnl = can\n",
    "    topics.append(fnl)\n",
    "\n",
    "\n",
    "print(topics)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relabel(text: str, subject: str):\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \n",
    "    \"content\": \"당신은 신문 기자입니다. 입력으로 신문 기사가 한 개가 들어오고 후보 주제둘이 주어집니다. 각 주제들은 개행 문자로 구분이 됩니다. 입력으로 들어온 후보 주제들 중에서 가장 신문 기사에 어울리는 주제 단 한 개를 출력하세요. 설명을 붙이지 말고 주제 하나만을 출력하세요. 복수의 주제를 출력하면 안됩니다.\"},\n",
    "    {\"role\": \"user\", \"content\": text},\n",
    "    {\"role\": \"user\", \"content\": subject}\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    outputs = llm.generate(\n",
    "        inputs.to(device),\n",
    "        max_new_tokens=4096,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(generated_text)\n",
    "\n",
    "    result = generated_text.split(\"[|assistant|]\")[-1].strip()\n",
    "    if '\\n' in result:\n",
    "        result = result.split(\"\\n\")[0]\n",
    "    del inputs, outputs, generated_text\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_topic = \"\"\n",
    "for subtopic in topics:\n",
    "    final_topic += subtopic + \"\\n\"\n",
    "\n",
    "with open(\"../data/original_nanoised.csv\", 'r') as f:\n",
    "    df2 = pd.read_csv(f)\n",
    "\n",
    "label_filtered = []\n",
    "for i, row in tqdm(df2.iterrows(), desc=\"Rerabelling\", total=len(df2)):\n",
    "    result = relabel(row['text'], final_topic)\n",
    "    idx = -1\n",
    "    for k, subtopic in enumerate(topics):\n",
    "        if subtopic == result:\n",
    "            idx = k\n",
    "            break\n",
    "    label_filtered.append(\n",
    "        {\n",
    "            'ID': row['ID'],\n",
    "            'text': row['text'],\n",
    "            'target': idx\n",
    "        }\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(label_filtered)\n",
    "df3.to_csv(\"../data/nanoised_label_filtered.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1181\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "print(len(df3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
